{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5cc9b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "# # CSCI544_HW2_JingyanPeng\n",
    "# - 09/26/2022\n",
    "\n",
    "#version python3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a4f8eba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import accuracy_score\n",
    "import torch\n",
    "CUDA_LAUNCH_BLOCKING = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b0554d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c29fb3d",
   "metadata": {},
   "source": [
    "# 1.Dataset Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6b1d12",
   "metadata": {},
   "source": [
    "- Build a Balanced Dataset Through Random Selection\n",
    "\n",
    "  Load the dataset and build a balanced dataset of 100K reviews along with their ratings to create labels through random selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "829d0005",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data.tsv', sep='\\t', on_bad_lines='skip')\n",
    "df['reviews'] = df['review_headline'] + ' ' + df['review_body']\n",
    "df['ratings'] = df['star_rating']\n",
    "df = df[['ratings','reviews']]\n",
    "df = df.dropna()\n",
    "\n",
    "s1=df[df.ratings == 1]\n",
    "s2=df[df.ratings == 2]\n",
    "s3=df[df.ratings == 3]\n",
    "s4=df[df.ratings == 4]\n",
    "s5=df[df.ratings == 5]\n",
    "s1 = s1.sample(n = 20000, random_state = None)\n",
    "s2 = s2.sample(n = 20000, random_state = None)\n",
    "s3 = s3.sample(n = 20000, random_state = None)\n",
    "s4 = s4.sample(n = 20000, random_state = None)\n",
    "s5 = s5.sample(n = 20000, random_state = None)\n",
    "dataset = pd.concat([s1, s2, s3, s4, s5])\n",
    "dataset = dataset.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5237c4c7",
   "metadata": {},
   "source": [
    "- Simple Data Cleaning without Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f318082e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ratings                                            reviews\n",
      "0       1  really low quality crap product turned orange ...\n",
      "1       1  waste of money unless you re buying them for a...\n",
      "2       1  one star not really what i wanted it s way to ...\n",
      "3       1  twice the price for half the amount this cost ...\n",
      "4     1.0  can t believe it i didn t like anything about ...\n"
     ]
    }
   ],
   "source": [
    "dataset['reviews'] = dataset['reviews'].str.lower()\n",
    "\n",
    "dataset['reviews'] = dataset['reviews'].map(lambda x: re.sub(re.compile(r'[http|https]*://[a-zA-Z0-9.?/&=:]*', re.S), \" \", x))\n",
    "dataset['reviews'] = dataset['reviews'].map(lambda x: BeautifulSoup(x,\"html.parser\").get_text())\n",
    "\n",
    "dataset['reviews'] = dataset['reviews'].map(lambda x: re.sub(\"[^a-zA-Z]+\", \" \", x))\n",
    "\n",
    "dataset['reviews'] = dataset['reviews'].map(lambda x: re.sub(r'\\s\\s+', ' ', x))\n",
    "#dataset['reviews'] = dataset['reviews'].map(lambda x: x.strip())\n",
    "\n",
    "import contractions\n",
    "def contractionFunc(s):\n",
    "    s = contractions.fix(s)\n",
    "    s = re.sub(\"[^a-zA-Z]+\", \" \", s)\n",
    "    return s\n",
    "dataset['reviews'] = dataset['reviews'].map(lambda x: contractionFunc(x))\n",
    "\n",
    "print(dataset.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcbc2db",
   "metadata": {},
   "source": [
    "# 2. Word Embedding\n",
    "- Reference:\n",
    "    - Gensim > Documentation > Word2Vec Model\n",
    "    - https://radimrehurek.com/gensim/auto_examples/tutorials/run_word2vec.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9332bf77",
   "metadata": {},
   "source": [
    "- (a) Pretrained Word2Vec Model\n",
    "\n",
    "Load the pretrained “word2vec-google-news-300” Word2Vec model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "20e30255",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "google_wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a19b6e4",
   "metadata": {},
   "source": [
    "Check semantic similarities of the generated vectors using wv.most_similar() & wv.similarity().\n",
    "\n",
    "My own three examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "163cc618",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('aunt', 0.8022665977478027)]\n",
      "0.85213083\n",
      "0.82077205\n"
     ]
    }
   ],
   "source": [
    "print(google_wv.most_similar(positive=['uncle', 'woman'], negative=['man'], topn=1))\n",
    "print(google_wv.similarity('bike', 'bicycle'))\n",
    "print(google_wv.similarity('crucial', 'vital'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a422e",
   "metadata": {},
   "source": [
    "Given two examples in assignment doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aae25658",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118193507194519)]\n",
      "0.55674857\n"
     ]
    }
   ],
   "source": [
    "print(google_wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))\n",
    "print(google_wv.similarity('excellent', 'outstanding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19561ed8",
   "metadata": {},
   "source": [
    "- (b) My Word2Vec Model\n",
    "\n",
    "Train a Word2Vec model using my own dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f74d31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "\n",
    "class MyCorpus:\n",
    "    def __iter__(self):\n",
    "        for line in dataset['reviews']:\n",
    "            yield utils.simple_preprocess(line)\n",
    "\n",
    "model = gensim.models.Word2Vec(sentences=MyCorpus(), vector_size=300, window=11, min_count=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9918efdb",
   "metadata": {},
   "source": [
    "Check semantic similarities of the generated vectors using wv.most_similar() & wv.similarity().\n",
    "\n",
    "Given two examples in assignment doc:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d187a3fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('poem', 0.5422888994216919)]\n",
      "0.81494856\n"
     ]
    }
   ],
   "source": [
    "print(model.wv.most_similar(positive=['king', 'woman'], negative=['man'], topn=1))\n",
    "print(model.wv.similarity('excellent', 'outstanding'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf254b90",
   "metadata": {},
   "source": [
    "- Conclusion\n",
    "    - It shows that for the example of 'King − Man + Woman = Queen', the “word2vec-google-news-300” Word2Vec model works better. The model trained with my own dataset cannot give the answer, 'queen'. But for the example of 'excellent ∼ outstanding'. The model trained with my own dataset works better (0.81 > 0.56).\n",
    "     \n",
    "    - This may be related to characteristics of different datasets. If the words appears often in the dataset, as 'excellent' and 'outstanding' is more commonly used in my own dataset than in google news dataset, the corresponding vectors of these commonly used words trained with this kind of dataset can be more accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b304ba62",
   "metadata": {},
   "source": [
    "# Train / Test split\n",
    "\n",
    "80%/20% training/testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a04ebff",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = dataset.sample(frac = 0.8, random_state = 1)\n",
    "test = dataset.drop(train.index)\n",
    "train = train.reset_index(drop = True)\n",
    "test = test.reset_index(drop = True)\n",
    "\n",
    "X_train = train['reviews']\n",
    "X_test = test['reviews']\n",
    "Y_train = train['ratings']\n",
    "Y_test = test['ratings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213a8699",
   "metadata": {},
   "source": [
    "# Function Definition for Word2Vec -> Input\n",
    "Here I define 5 functions to process the Word2Vec to input data.\n",
    "1. Delete the corresponding labels of the NaN training vectors.\n",
    "2. Delete the NaN training vectors.\n",
    "\n",
    "3. Calculate the average vector for each review.\n",
    "4. Concatenate the first 10 vectors for each review. Truncate the longer one and pad the shorter one with 0.\n",
    "5. VStack the first 20 vectors for each review. Truncate the longer one and pad the shorter one with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d4d7360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) processing NaN\n",
    "def process_nanY(x_mtx, y_mtx):\n",
    "    idx = []\n",
    "    if np.any(np.isnan(x_mtx)):\n",
    "        arr_nan = np.argwhere(np.isnan(x_mtx))\n",
    "        num_nan = arr_nan.shape[0]\n",
    "        arr = np.arange(0, num_nan, 300)\n",
    "        for i in arr:\n",
    "            idx.append(arr_nan[i][0])\n",
    "    if idx != None:\n",
    "        mtx = np.delete(y_mtx, idx)\n",
    "    return mtx;\n",
    "\n",
    "def process_nanX(x_mtx):\n",
    "    idx = []\n",
    "    if np.any(np.isnan(x_mtx)):\n",
    "        arr_nan = np.argwhere(np.isnan(x_mtx))\n",
    "        num_nan = arr_nan.shape[0]\n",
    "        arr = np.arange(0, num_nan, 300)\n",
    "        for i in arr:\n",
    "            idx.append(arr_nan[i][0])\n",
    "    if idx != None:\n",
    "        mtx = np.delete(x_mtx, idx, 0)\n",
    "    return mtx;\n",
    "\n",
    "\n",
    "# 2) the average Word2Vec vectors\n",
    "def w2v_average(wv_model, input_words):\n",
    "    wordlist = input_words.split(' ')\n",
    "    embed_sum = np.zeros(shape = (300,))\n",
    "    count = 0\n",
    "    for word in wordlist:\n",
    "        if word in wv_model:\n",
    "            embed_sum += wv_model[word]\n",
    "            count += 1\n",
    "    return embed_sum / count\n",
    "\n",
    "# 3) concatenate the first 10 Word2Vec vectors\n",
    "def w2v_first10(wv_model, input_words):\n",
    "    wordlist = input_words.split(' ')\n",
    "    idx = 0;\n",
    "    coun = 0;\n",
    "    # go through the reviews to find 10 words in W2V model\n",
    "    while(idx < len(wordlist)) & (coun <10):\n",
    "        if wordlist[idx] in wv_model:\n",
    "            wv_current = wv_model[wordlist[idx]]\n",
    "            if coun == 0:\n",
    "                result = wv_current\n",
    "            else:\n",
    "                result = np.concatenate((result, wv_current))\n",
    "            idx += 1\n",
    "            coun += 1\n",
    "        else:\n",
    "            idx +=1\n",
    "    # if reviews length < 10:\n",
    "    if coun == 0:\n",
    "        return np.zeros(shape = 3000, )\n",
    "    if coun < 10:\n",
    "        zeros = np.zeros(shape = (300 * (10 - coun), ))\n",
    "        return np.concatenate((result, zeros))\n",
    "    else:\n",
    "        return result\n",
    "    \n",
    "# 4) vStack the first 20 Word2Vec vectors\n",
    "#    limit the review length to 20 by truncating and padding\n",
    "def w2v_seq20(wv_model, input_words):\n",
    "    wordlist = input_words.split(' ')\n",
    "    idx = 0;\n",
    "    coun = 0;\n",
    "    # go through the reviews to find 10 words in W2V model\n",
    "    while(idx < len(wordlist)) & (coun < 20):\n",
    "        if wordlist[idx] in wv_model:\n",
    "            wv_current = wv_model[wordlist[idx]]\n",
    "            if coun == 0:\n",
    "                result = wv_current\n",
    "            else:\n",
    "                result = np.vstack((result, wv_current))\n",
    "            idx += 1\n",
    "            coun += 1\n",
    "        else:\n",
    "            idx +=1\n",
    "    # if reviews length < 20:\n",
    "    if coun == 0:\n",
    "        return np.zeros(shape = (20, 300) )\n",
    "    if coun < 20:\n",
    "        zeros = np.zeros(shape = (20 - coun, 300))\n",
    "        return np.vstack((result, zeros))\n",
    "    else:\n",
    "        return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad785216",
   "metadata": {},
   "source": [
    "# 3. Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1933c588",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculate the average Word2Vec vectors of Google-pretrained Word2Vec Model. \n",
    "X_train_preW2Vave =np.array(X_train.apply(lambda x: w2v_average(google_wv, x)).values.tolist())\n",
    "X_test_preW2Vave =np.array(X_test.apply(lambda x: w2v_average(google_wv, x)).values.tolist())\n",
    "Y_train_preW2Vave = np.array(Y_train.values.tolist())\n",
    "Y_test_preW2Vave = np.array(Y_test.values.tolist())\n",
    "#Delete NaN Vectors with their Labels.\n",
    "Y_train_preW2Vave = process_nanY(X_train_preW2Vave, Y_train_preW2Vave)\n",
    "X_train_preW2Vave = process_nanX(X_train_preW2Vave)\n",
    "Y_test_preW2Vave = process_nanY(X_test_preW2Vave, Y_test_preW2Vave)\n",
    "X_test_preW2Vave = process_nanX(X_test_preW2Vave)\n",
    "## print(X_train_preW2Vave.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff5736a",
   "metadata": {},
   "source": [
    "- Perceptron\n",
    "\n",
    "Use sklearn.linear_model.Perceptron().fit() to train a Perceptron model and sklearn.linear_model.Perceptron().predict() to get the accurancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "427c6311",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4594959495949595\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "perceptron_pre = Perceptron(max_iter = 1000, tol = 0, random_state = 0, eta0 = 0.01)\n",
    "perceptron_pre.fit(X_train_preW2Vave, Y_train_preW2Vave)\n",
    "perceptron_pre_test = perceptron_pre.predict(X_test_preW2Vave)\n",
    "perceptron_pre_test_accuracy = accuracy_score(Y_test_preW2Vave, perceptron_pre_test)\n",
    "print(perceptron_pre_test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a09fb1f3",
   "metadata": {},
   "source": [
    "- SVM\n",
    "\n",
    "Use sklearn.svm.LinearSVC().fit() to train a SVM model and sklearn.svm.LinearSVC().predict() to get the accurancy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5a6736b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5621562156215621\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svm_pre = LinearSVC(max_iter = 5000, random_state = 0)\n",
    "svm_pre.fit(X_train_preW2Vave, Y_train_preW2Vave)\n",
    "svm_pre_test = svm_pre.predict(X_test_preW2Vave)\n",
    "svm_pre_test_accurancy = accuracy_score(Y_test_preW2Vave, svm_pre_test)\n",
    "print(svm_pre_test_accurancy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5936f893",
   "metadata": {},
   "source": [
    "- Conclusion\n",
    "    - In HW1, I got the accurancy values of these simple models with TF-IDF as input——0.42 for Perceptron model and 0.44 for SVM model. They are all worse than the results using pretrained Word2Vec as input here——0.46 for Perceptron model and 0.56 for SVM model.\n",
    "    - So, in this case, the Word2Vec model works better than TF-IDF for words embedding."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426fc44",
   "metadata": {},
   "source": [
    "# Custom Dataset\n",
    "Define the dataset for Neural Network training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4265b11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "class Train(Dataset):\n",
    "    def __init__(self, xtrain, ytrain):\n",
    "        self.data = xtrain\n",
    "        self.labels = ytrain\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "class Test(Dataset):\n",
    "    def __init__(self, xtest, ytest):\n",
    "        self.data = xtest\n",
    "        self.labels = ytest\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        X = self.data[index]\n",
    "        y = self.labels[index]\n",
    "        return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54be9a73",
   "metadata": {},
   "source": [
    "# 4. Feedforward Neural Networks\n",
    "- Reference:\n",
    "    - Pytorch Multi-Layer Perceptron, MNIST | Author: BHARAT BUSHAN MISHRA\n",
    "    - https://www.kaggle.com/code/mishra1993/pytorch-multi-layer-perceptron-mnist/notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57c1f83",
   "metadata": {},
   "source": [
    "Define the Network Architecture (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "088e0be8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (fc1): Linear(in_features=300, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "MLP(\n",
      "  (fc1): Linear(in_features=3000, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "  (fc3): Linear(in_features=10, out_features=5, bias=True)\n",
      "  (dropout): Dropout(p=0.2, inplace=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the NN architecture \n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, D_input, D_output):\n",
    "        super(MLP, self).__init__()\n",
    "        # number of hidden nodes in each layer \n",
    "        # layer1: 50 nodes; layer2: 10 nodes\n",
    "        hidden_1 = 50\n",
    "        hidden_2 = 10\n",
    "        # linear layer (300 -> hidden_1)\n",
    "        self.fc1 = nn.Linear(D_input, hidden_1)\n",
    "        # linear layer (n_hidden -> hidden_2)\n",
    "        self.fc2 = nn.Linear(hidden_1, hidden_2)\n",
    "        # linear layer (n_hidden -> 5)\n",
    "        self.fc3 = nn.Linear(hidden_2, D_output)\n",
    "        # dropout layer (p=0.2)\n",
    "        # dropout prevents overfitting of data\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "    def forward(self, x):\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add hidden layer, with relu activation function\n",
    "        x = F.relu(self.fc2(x))\n",
    "        # add dropout layer\n",
    "        x = self.dropout(x)\n",
    "        # add output layer\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# initialize the NN\n",
    "model_4a = MLP(300, 5)\n",
    "model_4b = MLP(3000, 5)\n",
    "model_4a.cuda()\n",
    "model_4b.cuda()\n",
    "print(model_4a)\n",
    "print(model_4b)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer_4a = torch.optim.SGD(model_4a.parameters(), lr=0.01)\n",
    "optimizer_4b = torch.optim.SGD(model_4b.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f148ece2",
   "metadata": {},
   "source": [
    "- (a) the average Word2Vec vectors as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "587afe5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Load the data_4a ############################# \n",
    "train_data_4a = Train(X_train_preW2Vave, Y_train_preW2Vave-1)\n",
    "test_data_4a = Test(X_test_preW2Vave, Y_test_preW2Vave-1)\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_4a)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_4a = torch.utils.data.DataLoader(train_data_4a, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader_4a = torch.utils.data.DataLoader(train_data_4a, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader_4a = torch.utils.data.DataLoader(test_data_4a, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2bb547fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.291888 \tValidation Loss: 0.322190\n",
      "Validation loss decreased (inf --> 0.322190).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.287869 \tValidation Loss: 0.321915\n",
      "Validation loss decreased (0.322190 --> 0.321915).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.287442 \tValidation Loss: 0.321847\n",
      "Validation loss decreased (0.321915 --> 0.321847).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.287303 \tValidation Loss: 0.321805\n",
      "Validation loss decreased (0.321847 --> 0.321805).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.287180 \tValidation Loss: 0.321782\n",
      "Validation loss decreased (0.321805 --> 0.321782).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.287018 \tValidation Loss: 0.321735\n",
      "Validation loss decreased (0.321782 --> 0.321735).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.286869 \tValidation Loss: 0.321695\n",
      "Validation loss decreased (0.321735 --> 0.321695).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.286668 \tValidation Loss: 0.321644\n",
      "Validation loss decreased (0.321695 --> 0.321644).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.286455 \tValidation Loss: 0.321586\n",
      "Validation loss decreased (0.321644 --> 0.321586).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.286206 \tValidation Loss: 0.321513\n",
      "Validation loss decreased (0.321586 --> 0.321513).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.285757 \tValidation Loss: 0.321406\n",
      "Validation loss decreased (0.321513 --> 0.321406).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.285391 \tValidation Loss: 0.321282\n",
      "Validation loss decreased (0.321406 --> 0.321282).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.284819 \tValidation Loss: 0.321123\n",
      "Validation loss decreased (0.321282 --> 0.321123).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.284045 \tValidation Loss: 0.320900\n",
      "Validation loss decreased (0.321123 --> 0.320900).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.283021 \tValidation Loss: 0.320568\n",
      "Validation loss decreased (0.320900 --> 0.320568).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.281132 \tValidation Loss: 0.319953\n",
      "Validation loss decreased (0.320568 --> 0.319953).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.278506 \tValidation Loss: 0.319193\n",
      "Validation loss decreased (0.319953 --> 0.319193).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.274754 \tValidation Loss: 0.318010\n",
      "Validation loss decreased (0.319193 --> 0.318010).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.269149 \tValidation Loss: 0.315945\n",
      "Validation loss decreased (0.318010 --> 0.315945).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.259047 \tValidation Loss: 0.312754\n",
      "Validation loss decreased (0.315945 --> 0.312754).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.244182 \tValidation Loss: 0.307929\n",
      "Validation loss decreased (0.312754 --> 0.307929).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.224290 \tValidation Loss: 0.301651\n",
      "Validation loss decreased (0.307929 --> 0.301651).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.200268 \tValidation Loss: 0.294882\n",
      "Validation loss decreased (0.301651 --> 0.294882).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.177957 \tValidation Loss: 0.288566\n",
      "Validation loss decreased (0.294882 --> 0.288566).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.156982 \tValidation Loss: 0.282964\n",
      "Validation loss decreased (0.288566 --> 0.282964).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.137607 \tValidation Loss: 0.277914\n",
      "Validation loss decreased (0.282964 --> 0.277914).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.120865 \tValidation Loss: 0.273507\n",
      "Validation loss decreased (0.277914 --> 0.273507).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.106743 \tValidation Loss: 0.269681\n",
      "Validation loss decreased (0.273507 --> 0.269681).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.094160 \tValidation Loss: 0.266199\n",
      "Validation loss decreased (0.269681 --> 0.266199).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.080650 \tValidation Loss: 0.263057\n",
      "Validation loss decreased (0.266199 --> 0.263057).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.072850 \tValidation Loss: 0.260479\n",
      "Validation loss decreased (0.263057 --> 0.260479).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.061878 \tValidation Loss: 0.258121\n",
      "Validation loss decreased (0.260479 --> 0.258121).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.053866 \tValidation Loss: 0.255661\n",
      "Validation loss decreased (0.258121 --> 0.255661).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.044948 \tValidation Loss: 0.253541\n",
      "Validation loss decreased (0.255661 --> 0.253541).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.037253 \tValidation Loss: 0.251743\n",
      "Validation loss decreased (0.253541 --> 0.251743).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.030884 \tValidation Loss: 0.250129\n",
      "Validation loss decreased (0.251743 --> 0.250129).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.023242 \tValidation Loss: 0.248392\n",
      "Validation loss decreased (0.250129 --> 0.248392).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.019616 \tValidation Loss: 0.246983\n",
      "Validation loss decreased (0.248392 --> 0.246983).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.013652 \tValidation Loss: 0.245626\n",
      "Validation loss decreased (0.246983 --> 0.245626).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.008268 \tValidation Loss: 0.244319\n",
      "Validation loss decreased (0.245626 --> 0.244319).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.004315 \tValidation Loss: 0.243117\n",
      "Validation loss decreased (0.244319 --> 0.243117).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.001012 \tValidation Loss: 0.241775\n",
      "Validation loss decreased (0.243117 --> 0.241775).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.994878 \tValidation Loss: 0.240578\n",
      "Validation loss decreased (0.241775 --> 0.240578).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.991673 \tValidation Loss: 0.239592\n",
      "Validation loss decreased (0.240578 --> 0.239592).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 0.986597 \tValidation Loss: 0.238503\n",
      "Validation loss decreased (0.239592 --> 0.238503).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.984068 \tValidation Loss: 0.237756\n",
      "Validation loss decreased (0.238503 --> 0.237756).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.979601 \tValidation Loss: 0.236679\n",
      "Validation loss decreased (0.237756 --> 0.236679).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.976399 \tValidation Loss: 0.235866\n",
      "Validation loss decreased (0.236679 --> 0.235866).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.973594 \tValidation Loss: 0.235080\n",
      "Validation loss decreased (0.235866 --> 0.235080).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 0.969694 \tValidation Loss: 0.234169\n",
      "Validation loss decreased (0.235080 --> 0.234169).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 0.968368 \tValidation Loss: 0.233687\n",
      "Validation loss decreased (0.234169 --> 0.233687).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 0.964001 \tValidation Loss: 0.232724\n",
      "Validation loss decreased (0.233687 --> 0.232724).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 0.962887 \tValidation Loss: 0.232101\n",
      "Validation loss decreased (0.232724 --> 0.232101).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 0.958259 \tValidation Loss: 0.231742\n",
      "Validation loss decreased (0.232101 --> 0.231742).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 0.958435 \tValidation Loss: 0.230670\n",
      "Validation loss decreased (0.231742 --> 0.230670).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 0.953616 \tValidation Loss: 0.230265\n",
      "Validation loss decreased (0.230670 --> 0.230265).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 0.951956 \tValidation Loss: 0.229575\n",
      "Validation loss decreased (0.230265 --> 0.229575).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 0.949562 \tValidation Loss: 0.228891\n",
      "Validation loss decreased (0.229575 --> 0.228891).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 0.947658 \tValidation Loss: 0.228967\n",
      "Epoch: 60 \tTraining Loss: 0.944441 \tValidation Loss: 0.228231\n",
      "Validation loss decreased (0.228891 --> 0.228231).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 0.943739 \tValidation Loss: 0.227487\n",
      "Validation loss decreased (0.228231 --> 0.227487).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 0.940192 \tValidation Loss: 0.226727\n",
      "Validation loss decreased (0.227487 --> 0.226727).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 0.939797 \tValidation Loss: 0.226221\n",
      "Validation loss decreased (0.226727 --> 0.226221).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 0.937761 \tValidation Loss: 0.225837\n",
      "Validation loss decreased (0.226221 --> 0.225837).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 0.934809 \tValidation Loss: 0.225310\n",
      "Validation loss decreased (0.225837 --> 0.225310).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 0.934270 \tValidation Loss: 0.224736\n",
      "Validation loss decreased (0.225310 --> 0.224736).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.932015 \tValidation Loss: 0.224418\n",
      "Validation loss decreased (0.224736 --> 0.224418).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 0.928870 \tValidation Loss: 0.223870\n",
      "Validation loss decreased (0.224418 --> 0.223870).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 0.926700 \tValidation Loss: 0.223305\n",
      "Validation loss decreased (0.223870 --> 0.223305).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.924372 \tValidation Loss: 0.222808\n",
      "Validation loss decreased (0.223305 --> 0.222808).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 0.923455 \tValidation Loss: 0.222419\n",
      "Validation loss decreased (0.222808 --> 0.222419).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 0.922550 \tValidation Loss: 0.221885\n",
      "Validation loss decreased (0.222419 --> 0.221885).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 0.919910 \tValidation Loss: 0.221446\n",
      "Validation loss decreased (0.221885 --> 0.221446).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 0.918084 \tValidation Loss: 0.221226\n",
      "Validation loss decreased (0.221446 --> 0.221226).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 0.915768 \tValidation Loss: 0.221101\n",
      "Validation loss decreased (0.221226 --> 0.221101).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 0.914186 \tValidation Loss: 0.220374\n",
      "Validation loss decreased (0.221101 --> 0.220374).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.913734 \tValidation Loss: 0.219936\n",
      "Validation loss decreased (0.220374 --> 0.219936).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 0.910765 \tValidation Loss: 0.219452\n",
      "Validation loss decreased (0.219936 --> 0.219452).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.909278 \tValidation Loss: 0.219036\n",
      "Validation loss decreased (0.219452 --> 0.219036).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.907350 \tValidation Loss: 0.218699\n",
      "Validation loss decreased (0.219036 --> 0.218699).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 0.906042 \tValidation Loss: 0.218240\n",
      "Validation loss decreased (0.218699 --> 0.218240).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 0.903571 \tValidation Loss: 0.217489\n",
      "Validation loss decreased (0.218240 --> 0.217489).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.901780 \tValidation Loss: 0.217059\n",
      "Validation loss decreased (0.217489 --> 0.217059).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 0.900054 \tValidation Loss: 0.216852\n",
      "Validation loss decreased (0.217059 --> 0.216852).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.900946 \tValidation Loss: 0.217625\n",
      "Epoch: 86 \tTraining Loss: 0.898363 \tValidation Loss: 0.216348\n",
      "Validation loss decreased (0.216852 --> 0.216348).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 0.896153 \tValidation Loss: 0.215803\n",
      "Validation loss decreased (0.216348 --> 0.215803).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 0.895205 \tValidation Loss: 0.215502\n",
      "Validation loss decreased (0.215803 --> 0.215502).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 0.893839 \tValidation Loss: 0.214995\n",
      "Validation loss decreased (0.215502 --> 0.214995).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 0.894365 \tValidation Loss: 0.215363\n",
      "Epoch: 91 \tTraining Loss: 0.891835 \tValidation Loss: 0.214529\n",
      "Validation loss decreased (0.214995 --> 0.214529).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 0.890510 \tValidation Loss: 0.214373\n",
      "Validation loss decreased (0.214529 --> 0.214373).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 0.887154 \tValidation Loss: 0.214060\n",
      "Validation loss decreased (0.214373 --> 0.214060).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 0.888484 \tValidation Loss: 0.213805\n",
      "Validation loss decreased (0.214060 --> 0.213805).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 0.887158 \tValidation Loss: 0.213733\n",
      "Validation loss decreased (0.213805 --> 0.213733).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 0.885518 \tValidation Loss: 0.213589\n",
      "Validation loss decreased (0.213733 --> 0.213589).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 0.883553 \tValidation Loss: 0.212451\n",
      "Validation loss decreased (0.213589 --> 0.212451).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 0.882370 \tValidation Loss: 0.212272\n",
      "Validation loss decreased (0.212451 --> 0.212272).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 0.881347 \tValidation Loss: 0.212382\n",
      "Epoch: 100 \tTraining Loss: 0.880502 \tValidation Loss: 0.212003\n",
      "Validation loss decreased (0.212272 --> 0.212003).  Saving model ...\n",
      "Epoch: 101 \tTraining Loss: 0.879786 \tValidation Loss: 0.211565\n",
      "Validation loss decreased (0.212003 --> 0.211565).  Saving model ...\n",
      "Epoch: 102 \tTraining Loss: 0.877767 \tValidation Loss: 0.211241\n",
      "Validation loss decreased (0.211565 --> 0.211241).  Saving model ...\n",
      "Epoch: 103 \tTraining Loss: 0.874903 \tValidation Loss: 0.210906\n",
      "Validation loss decreased (0.211241 --> 0.210906).  Saving model ...\n",
      "Epoch: 104 \tTraining Loss: 0.877778 \tValidation Loss: 0.211165\n",
      "Epoch: 105 \tTraining Loss: 0.875874 \tValidation Loss: 0.211264\n",
      "Epoch: 106 \tTraining Loss: 0.875490 \tValidation Loss: 0.210402\n",
      "Validation loss decreased (0.210906 --> 0.210402).  Saving model ...\n",
      "Epoch: 107 \tTraining Loss: 0.874646 \tValidation Loss: 0.210697\n",
      "Epoch: 108 \tTraining Loss: 0.872720 \tValidation Loss: 0.210000\n",
      "Validation loss decreased (0.210402 --> 0.210000).  Saving model ...\n",
      "Epoch: 109 \tTraining Loss: 0.872451 \tValidation Loss: 0.209605\n",
      "Validation loss decreased (0.210000 --> 0.209605).  Saving model ...\n",
      "Epoch: 110 \tTraining Loss: 0.870960 \tValidation Loss: 0.209950\n",
      "Epoch: 111 \tTraining Loss: 0.869272 \tValidation Loss: 0.209404\n",
      "Validation loss decreased (0.209605 --> 0.209404).  Saving model ...\n",
      "Epoch: 112 \tTraining Loss: 0.869148 \tValidation Loss: 0.208757\n",
      "Validation loss decreased (0.209404 --> 0.208757).  Saving model ...\n",
      "Epoch: 113 \tTraining Loss: 0.867739 \tValidation Loss: 0.209077\n",
      "Epoch: 114 \tTraining Loss: 0.865680 \tValidation Loss: 0.208785\n",
      "Epoch: 115 \tTraining Loss: 0.864672 \tValidation Loss: 0.209393\n",
      "Epoch: 116 \tTraining Loss: 0.863473 \tValidation Loss: 0.208004\n",
      "Validation loss decreased (0.208757 --> 0.208004).  Saving model ...\n",
      "Epoch: 117 \tTraining Loss: 0.863373 \tValidation Loss: 0.208034\n",
      "Epoch: 118 \tTraining Loss: 0.862995 \tValidation Loss: 0.207768\n",
      "Validation loss decreased (0.208004 --> 0.207768).  Saving model ...\n",
      "Epoch: 119 \tTraining Loss: 0.862538 \tValidation Loss: 0.208044\n",
      "Epoch: 120 \tTraining Loss: 0.862091 \tValidation Loss: 0.207252\n",
      "Validation loss decreased (0.207768 --> 0.207252).  Saving model ...\n",
      "Epoch: 121 \tTraining Loss: 0.860212 \tValidation Loss: 0.207692\n",
      "Epoch: 122 \tTraining Loss: 0.859222 \tValidation Loss: 0.206571\n",
      "Validation loss decreased (0.207252 --> 0.206571).  Saving model ...\n",
      "Epoch: 123 \tTraining Loss: 0.859297 \tValidation Loss: 0.207450\n",
      "Epoch: 124 \tTraining Loss: 0.856474 \tValidation Loss: 0.207396\n",
      "Epoch: 125 \tTraining Loss: 0.857119 \tValidation Loss: 0.206671\n",
      "Epoch: 126 \tTraining Loss: 0.857831 \tValidation Loss: 0.206400\n",
      "Validation loss decreased (0.206571 --> 0.206400).  Saving model ...\n",
      "Epoch: 127 \tTraining Loss: 0.854012 \tValidation Loss: 0.206006\n",
      "Validation loss decreased (0.206400 --> 0.206006).  Saving model ...\n",
      "Epoch: 128 \tTraining Loss: 0.853439 \tValidation Loss: 0.205778\n",
      "Validation loss decreased (0.206006 --> 0.205778).  Saving model ...\n",
      "Epoch: 129 \tTraining Loss: 0.855424 \tValidation Loss: 0.205610\n",
      "Validation loss decreased (0.205778 --> 0.205610).  Saving model ...\n",
      "Epoch: 130 \tTraining Loss: 0.851856 \tValidation Loss: 0.205756\n",
      "Epoch: 131 \tTraining Loss: 0.851434 \tValidation Loss: 0.205709\n",
      "Epoch: 132 \tTraining Loss: 0.850421 \tValidation Loss: 0.204863\n",
      "Validation loss decreased (0.205610 --> 0.204863).  Saving model ...\n",
      "Epoch: 133 \tTraining Loss: 0.849895 \tValidation Loss: 0.205686\n",
      "Epoch: 134 \tTraining Loss: 0.851511 \tValidation Loss: 0.204852\n",
      "Validation loss decreased (0.204863 --> 0.204852).  Saving model ...\n",
      "Epoch: 135 \tTraining Loss: 0.847761 \tValidation Loss: 0.204418\n",
      "Validation loss decreased (0.204852 --> 0.204418).  Saving model ...\n",
      "Epoch: 136 \tTraining Loss: 0.847176 \tValidation Loss: 0.204241\n",
      "Validation loss decreased (0.204418 --> 0.204241).  Saving model ...\n",
      "Epoch: 137 \tTraining Loss: 0.846153 \tValidation Loss: 0.206336\n",
      "Epoch: 138 \tTraining Loss: 0.846038 \tValidation Loss: 0.204056\n",
      "Validation loss decreased (0.204241 --> 0.204056).  Saving model ...\n",
      "Epoch: 139 \tTraining Loss: 0.846941 \tValidation Loss: 0.203407\n",
      "Validation loss decreased (0.204056 --> 0.203407).  Saving model ...\n",
      "Epoch: 140 \tTraining Loss: 0.844332 \tValidation Loss: 0.204351\n",
      "Epoch: 141 \tTraining Loss: 0.844347 \tValidation Loss: 0.203626\n",
      "Epoch: 142 \tTraining Loss: 0.842002 \tValidation Loss: 0.203224\n",
      "Validation loss decreased (0.203407 --> 0.203224).  Saving model ...\n",
      "Epoch: 143 \tTraining Loss: 0.842229 \tValidation Loss: 0.202874\n",
      "Validation loss decreased (0.203224 --> 0.202874).  Saving model ...\n",
      "Epoch: 144 \tTraining Loss: 0.840950 \tValidation Loss: 0.202543\n",
      "Validation loss decreased (0.202874 --> 0.202543).  Saving model ...\n",
      "Epoch: 145 \tTraining Loss: 0.842863 \tValidation Loss: 0.202679\n",
      "Epoch: 146 \tTraining Loss: 0.843186 \tValidation Loss: 0.203025\n",
      "Epoch: 147 \tTraining Loss: 0.839189 \tValidation Loss: 0.202223\n",
      "Validation loss decreased (0.202543 --> 0.202223).  Saving model ...\n",
      "Epoch: 148 \tTraining Loss: 0.839394 \tValidation Loss: 0.202036\n",
      "Validation loss decreased (0.202223 --> 0.202036).  Saving model ...\n",
      "Epoch: 149 \tTraining Loss: 0.837752 \tValidation Loss: 0.202530\n",
      "Epoch: 150 \tTraining Loss: 0.838405 \tValidation Loss: 0.202111\n",
      "Time elapsed: 228.71 s\n"
     ]
    }
   ],
   "source": [
    "############################# Train the model_4a ############################# \n",
    "start = time.time()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 150\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model_4a.train() # prep model for training\n",
    "    for data, target in train_loader_4a:\n",
    "        # transfer data and target to GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_4a.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_4a(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(torch.long))\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_4a.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model_4a.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader_4a:\n",
    "        # transfer data and target to GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_4a(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(torch.long))\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader_4a.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader_4a.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model_4a.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        \n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "80b87f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5713071307130713\n"
     ]
    }
   ],
   "source": [
    "#############################  Calculate & Print the Accurancy_4a on Test############################# \n",
    "# Load the model with the lowest validation loss\n",
    "model_4a.load_state_dict(torch.load('model.pt'))\n",
    "# Calculate the accurancy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader_4a:\n",
    "        embeddings, labels = data\n",
    "        # calculating outputs by running embeddings through the network\n",
    "        model_4a.to(\"cpu\")\n",
    "        outputs = model_4a(embeddings.float())\n",
    "        # the class with the highest score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "324da42c",
   "metadata": {},
   "source": [
    "- (b) concatenate the first 10 Word2Vec vectors as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ae2c3023",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenate the first 10 Word2Vec vectors of Google-pretrained Word2Vec Model. \n",
    "X_train_preW2Vfirst10 =np.array(X_train.apply(lambda x: w2v_first10(google_wv, x)).values.tolist())\n",
    "X_test_preW2Vfirst10 =np.array(X_test.apply(lambda x: w2v_first10(google_wv, x)).values.tolist())\n",
    "Y_train_preW2Vfirst10 = np.array(Y_train.values.tolist())\n",
    "Y_test_preW2Vfirst10 = np.array(Y_test.values.tolist())\n",
    "#Delete NaN Vectors with their Labels\n",
    "Y_train_preW2Vfirst10 = process_nanY(X_train_preW2Vfirst10, Y_train_preW2Vfirst10)\n",
    "X_train_preW2Vfirst10 = process_nanX(X_train_preW2Vfirst10)\n",
    "Y_test_preW2Vfirst10 = process_nanY(X_test_preW2Vfirst10, Y_test_preW2Vfirst10)\n",
    "X_test_preW2Vfirst10 = process_nanX(X_test_preW2Vfirst10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b470335d",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Load the data_4b ############################# \n",
    "train_data_4b = Train(X_train_preW2Vfirst10, Y_train_preW2Vfirst10-1)\n",
    "test_data_4b = Test(X_test_preW2Vfirst10, Y_test_preW2Vfirst10-1)\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 100\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_4b)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_4b = torch.utils.data.DataLoader(train_data_4b, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader_4b = torch.utils.data.DataLoader(train_data_4b, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader_4b = torch.utils.data.DataLoader(test_data_4b, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53f936dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.289511 \tValidation Loss: 0.320628\n",
      "Validation loss decreased (inf --> 0.320628).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.274435 \tValidation Loss: 0.315348\n",
      "Validation loss decreased (0.320628 --> 0.315348).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.234018 \tValidation Loss: 0.296562\n",
      "Validation loss decreased (0.315348 --> 0.296562).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.145433 \tValidation Loss: 0.270334\n",
      "Validation loss decreased (0.296562 --> 0.270334).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.074494 \tValidation Loss: 0.256175\n",
      "Validation loss decreased (0.270334 --> 0.256175).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.036164 \tValidation Loss: 0.248015\n",
      "Validation loss decreased (0.256175 --> 0.248015).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.010050 \tValidation Loss: 0.242429\n",
      "Validation loss decreased (0.248015 --> 0.242429).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.986314 \tValidation Loss: 0.237203\n",
      "Validation loss decreased (0.242429 --> 0.237203).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 0.968403 \tValidation Loss: 0.233014\n",
      "Validation loss decreased (0.237203 --> 0.233014).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 0.950950 \tValidation Loss: 0.229083\n",
      "Validation loss decreased (0.233014 --> 0.229083).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 0.936779 \tValidation Loss: 0.225619\n",
      "Validation loss decreased (0.229083 --> 0.225619).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 0.923480 \tValidation Loss: 0.222807\n",
      "Validation loss decreased (0.225619 --> 0.222807).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.910284 \tValidation Loss: 0.219944\n",
      "Validation loss decreased (0.222807 --> 0.219944).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 0.899857 \tValidation Loss: 0.217853\n",
      "Validation loss decreased (0.219944 --> 0.217853).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.889516 \tValidation Loss: 0.214809\n",
      "Validation loss decreased (0.217853 --> 0.214809).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 0.878222 \tValidation Loss: 0.212580\n",
      "Validation loss decreased (0.214809 --> 0.212580).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 0.865779 \tValidation Loss: 0.210392\n",
      "Validation loss decreased (0.212580 --> 0.210392).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 0.857747 \tValidation Loss: 0.208768\n",
      "Validation loss decreased (0.210392 --> 0.208768).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.849002 \tValidation Loss: 0.207060\n",
      "Validation loss decreased (0.208768 --> 0.207060).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 0.842809 \tValidation Loss: 0.205862\n",
      "Validation loss decreased (0.207060 --> 0.205862).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.833960 \tValidation Loss: 0.204378\n",
      "Validation loss decreased (0.205862 --> 0.204378).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 0.825804 \tValidation Loss: 0.203159\n",
      "Validation loss decreased (0.204378 --> 0.203159).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 0.820146 \tValidation Loss: 0.202269\n",
      "Validation loss decreased (0.203159 --> 0.202269).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 0.812477 \tValidation Loss: 0.201148\n",
      "Validation loss decreased (0.202269 --> 0.201148).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 0.805287 \tValidation Loss: 0.199896\n",
      "Validation loss decreased (0.201148 --> 0.199896).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 0.798398 \tValidation Loss: 0.199438\n",
      "Validation loss decreased (0.199896 --> 0.199438).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 0.789745 \tValidation Loss: 0.198631\n",
      "Validation loss decreased (0.199438 --> 0.198631).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 0.787119 \tValidation Loss: 0.198014\n",
      "Validation loss decreased (0.198631 --> 0.198014).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 0.779115 \tValidation Loss: 0.197337\n",
      "Validation loss decreased (0.198014 --> 0.197337).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 0.771816 \tValidation Loss: 0.197119\n",
      "Validation loss decreased (0.197337 --> 0.197119).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 0.767442 \tValidation Loss: 0.196465\n",
      "Validation loss decreased (0.197119 --> 0.196465).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 0.761873 \tValidation Loss: 0.195609\n",
      "Validation loss decreased (0.196465 --> 0.195609).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 0.754038 \tValidation Loss: 0.195417\n",
      "Validation loss decreased (0.195609 --> 0.195417).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 0.752689 \tValidation Loss: 0.195509\n",
      "Epoch: 35 \tTraining Loss: 0.742244 \tValidation Loss: 0.194678\n",
      "Validation loss decreased (0.195417 --> 0.194678).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 0.737640 \tValidation Loss: 0.194497\n",
      "Validation loss decreased (0.194678 --> 0.194497).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 0.733939 \tValidation Loss: 0.194687\n",
      "Epoch: 38 \tTraining Loss: 0.727856 \tValidation Loss: 0.194186\n",
      "Validation loss decreased (0.194497 --> 0.194186).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 0.723471 \tValidation Loss: 0.194380\n",
      "Epoch: 40 \tTraining Loss: 0.717033 \tValidation Loss: 0.194276\n",
      "Epoch: 41 \tTraining Loss: 0.713816 \tValidation Loss: 0.194513\n",
      "Epoch: 42 \tTraining Loss: 0.707955 \tValidation Loss: 0.194039\n",
      "Validation loss decreased (0.194186 --> 0.194039).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 0.701363 \tValidation Loss: 0.194603\n",
      "Epoch: 44 \tTraining Loss: 0.698734 \tValidation Loss: 0.194561\n",
      "Epoch: 45 \tTraining Loss: 0.694149 \tValidation Loss: 0.194624\n",
      "Epoch: 46 \tTraining Loss: 0.689464 \tValidation Loss: 0.194634\n",
      "Epoch: 47 \tTraining Loss: 0.682761 \tValidation Loss: 0.195063\n",
      "Epoch: 48 \tTraining Loss: 0.679266 \tValidation Loss: 0.195177\n",
      "Epoch: 49 \tTraining Loss: 0.676229 \tValidation Loss: 0.195622\n",
      "Epoch: 50 \tTraining Loss: 0.666042 \tValidation Loss: 0.195554\n",
      "Epoch: 51 \tTraining Loss: 0.665183 \tValidation Loss: 0.196092\n",
      "Epoch: 52 \tTraining Loss: 0.659861 \tValidation Loss: 0.196293\n",
      "Epoch: 53 \tTraining Loss: 0.653907 \tValidation Loss: 0.196925\n",
      "Epoch: 54 \tTraining Loss: 0.649092 \tValidation Loss: 0.196802\n",
      "Epoch: 55 \tTraining Loss: 0.646694 \tValidation Loss: 0.198523\n",
      "Epoch: 56 \tTraining Loss: 0.641877 \tValidation Loss: 0.198493\n",
      "Epoch: 57 \tTraining Loss: 0.637907 \tValidation Loss: 0.198670\n",
      "Epoch: 58 \tTraining Loss: 0.634346 \tValidation Loss: 0.198763\n",
      "Epoch: 59 \tTraining Loss: 0.629364 \tValidation Loss: 0.199607\n",
      "Epoch: 60 \tTraining Loss: 0.623246 \tValidation Loss: 0.200012\n",
      "Time elapsed: 131.32 s\n"
     ]
    }
   ],
   "source": [
    "############################# Train the model_4b ############################# \n",
    "start = time.time()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 60\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model_4b.train() # prep model for training\n",
    "    for data, target in train_loader_4b:\n",
    "        # transfer data and target to GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_4b.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_4b(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(torch.long))\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_4b.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model_4b.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader_4b:\n",
    "        # transfer data and target to GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_4b(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(torch.long))\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader_4b.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader_4b.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model_4b.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        \n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "2b1087c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5851\n"
     ]
    }
   ],
   "source": [
    "#############################  Calculate & Print the Accurancy_4b on Test############################# \n",
    "# Load the model with the lowest validation loss\n",
    "model_4b.load_state_dict(torch.load('model.pt'))\n",
    "# Calculate the accurancy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader_4b:\n",
    "        embeddings, labels = data\n",
    "        # calculating outputs by running embeddings through the network\n",
    "        model_4b.to(\"cpu\")\n",
    "        outputs = model_4b(embeddings.float())\n",
    "        # the class with the highest score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85d9406c",
   "metadata": {},
   "source": [
    "- Conclusion\n",
    "    - FNN model obviously works better than simple models with average vectors as training data.\n",
    "    - Using the first 10 concatenated vectors as input is better than using the average vectors. The possible reason may be the loss of information when using average vectors. In addition, in most reviews, the first 10 words can correctly determine the classification result of this review."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81098090",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91caa50",
   "metadata": {},
   "source": [
    "- Define the Network Architecture (RNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "283ebc0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RNN(\n",
      "  (rnn): RNN(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n",
      "RNN(\n",
      "  (rnn): GRU(300, 20, batch_first=True)\n",
      "  (fc): Linear(in_features=20, out_features=5, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# define the RNN/GRN architecture \n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, model_type = \"rnn\"):\n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        # define the RNN's parameters\n",
    "        self.hidden_dim = 20\n",
    "        self.n_layers = 1\n",
    "        self.model_type = model_type\n",
    "        \n",
    "        #RNN\n",
    "        if self.model_type == \"gru\":\n",
    "            self.rnn = nn.GRU(300, 20, 1, batch_first=True)\n",
    "        else:\n",
    "            self.rnn = nn.RNN(300, 20, 1, batch_first=True, nonlinearity='relu')\n",
    "\n",
    "        #Outpur layer\n",
    "        self.fc = nn.Linear(20, 5)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden state with zeros\n",
    "        h0 = torch.zeros(1, x.size(0), 20).to(device)\n",
    "    \n",
    "        # One time step\n",
    "        out, hn = self.rnn(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# initialize the NN\n",
    "model_5a = RNN(model_type = \"rnn\")\n",
    "model_5b = RNN(model_type = \"gru\")\n",
    "model_5a.cuda()\n",
    "model_5b.cuda()\n",
    "print(model_5a)\n",
    "print(model_5b)\n",
    "\n",
    "# specify loss function (categorical cross-entropy)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# specify optimizer (stochastic gradient descent) and learning rate = 0.01\n",
    "optimizer_5a = torch.optim.SGD(model_5a.parameters(), lr=0.01)\n",
    "optimizer_5b = torch.optim.SGD(model_5b.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "afe4377f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vStack the first 20 Word2Vec vectors of Google-pretrained Word2Vec Model with trancating & padding \n",
    "X_train_preW2Vseq20 =np.array(X_train.apply(lambda x: w2v_seq20(google_wv, x)).values.tolist())\n",
    "X_test_preW2Vseq20 =np.array(X_test.apply(lambda x: w2v_seq20(google_wv, x)).values.tolist())\n",
    "Y_train_preW2Vseq20 = np.array(Y_train.values.tolist())\n",
    "Y_test_preW2Vseq20 = np.array(Y_test.values.tolist())\n",
    "#Delete NaN Vectors with their Labels\n",
    "Y_train_preW2Vseq20 = process_nanY(X_train_preW2Vseq20, Y_train_preW2Vseq20)\n",
    "X_train_preW2Vseq20 = process_nanX(X_train_preW2Vseq20)\n",
    "Y_test_preW2Vseq20 = process_nanY(X_test_preW2Vseq20, Y_test_preW2Vseq20)\n",
    "X_test_preW2Vseq20 = process_nanX(X_test_preW2Vseq20)\n",
    "#print(X_train_preW2Vseq20.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f361d3",
   "metadata": {},
   "source": [
    "- (a) Train a simple RNN for sentiment analysis, limiting the review length to 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63444428",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Load the data_5a ############################# \n",
    "train_data_5a = Train(X_train_preW2Vseq20, Y_train_preW2Vseq20-1)\n",
    "test_data_5a = Test(X_test_preW2Vseq20, Y_test_preW2Vseq20-1)\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 500\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_5a)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_5a = torch.utils.data.DataLoader(train_data_5a, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader_5a = torch.utils.data.DataLoader(train_data_5a, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader_5a = torch.utils.data.DataLoader(test_data_5a, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "751f35a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.292324 \tValidation Loss: 0.322846\n",
      "Validation loss decreased (inf --> 0.322846).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.290458 \tValidation Loss: 0.322543\n",
      "Validation loss decreased (0.322846 --> 0.322543).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.289515 \tValidation Loss: 0.322386\n",
      "Validation loss decreased (0.322543 --> 0.322386).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.288990 \tValidation Loss: 0.322294\n",
      "Validation loss decreased (0.322386 --> 0.322294).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.288644 \tValidation Loss: 0.322227\n",
      "Validation loss decreased (0.322294 --> 0.322227).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.288379 \tValidation Loss: 0.322172\n",
      "Validation loss decreased (0.322227 --> 0.322172).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.288153 \tValidation Loss: 0.322122\n",
      "Validation loss decreased (0.322172 --> 0.322122).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.287949 \tValidation Loss: 0.322075\n",
      "Validation loss decreased (0.322122 --> 0.322075).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.287757 \tValidation Loss: 0.322032\n",
      "Validation loss decreased (0.322075 --> 0.322032).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.287571 \tValidation Loss: 0.321989\n",
      "Validation loss decreased (0.322032 --> 0.321989).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.287394 \tValidation Loss: 0.321948\n",
      "Validation loss decreased (0.321989 --> 0.321948).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.287220 \tValidation Loss: 0.321907\n",
      "Validation loss decreased (0.321948 --> 0.321907).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.287047 \tValidation Loss: 0.321868\n",
      "Validation loss decreased (0.321907 --> 0.321868).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.286879 \tValidation Loss: 0.321827\n",
      "Validation loss decreased (0.321868 --> 0.321827).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.286709 \tValidation Loss: 0.321788\n",
      "Validation loss decreased (0.321827 --> 0.321788).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.286538 \tValidation Loss: 0.321747\n",
      "Validation loss decreased (0.321788 --> 0.321747).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.286371 \tValidation Loss: 0.321707\n",
      "Validation loss decreased (0.321747 --> 0.321707).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.286202 \tValidation Loss: 0.321666\n",
      "Validation loss decreased (0.321707 --> 0.321666).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.286034 \tValidation Loss: 0.321625\n",
      "Validation loss decreased (0.321666 --> 0.321625).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.285867 \tValidation Loss: 0.321583\n",
      "Validation loss decreased (0.321625 --> 0.321583).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.285694 \tValidation Loss: 0.321541\n",
      "Validation loss decreased (0.321583 --> 0.321541).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.285519 \tValidation Loss: 0.321499\n",
      "Validation loss decreased (0.321541 --> 0.321499).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.285342 \tValidation Loss: 0.321454\n",
      "Validation loss decreased (0.321499 --> 0.321454).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.285158 \tValidation Loss: 0.321409\n",
      "Validation loss decreased (0.321454 --> 0.321409).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.284972 \tValidation Loss: 0.321361\n",
      "Validation loss decreased (0.321409 --> 0.321361).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.284779 \tValidation Loss: 0.321314\n",
      "Validation loss decreased (0.321361 --> 0.321314).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.284585 \tValidation Loss: 0.321264\n",
      "Validation loss decreased (0.321314 --> 0.321264).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.284384 \tValidation Loss: 0.321213\n",
      "Validation loss decreased (0.321264 --> 0.321213).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.284175 \tValidation Loss: 0.321160\n",
      "Validation loss decreased (0.321213 --> 0.321160).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.283958 \tValidation Loss: 0.321107\n",
      "Validation loss decreased (0.321160 --> 0.321107).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.283736 \tValidation Loss: 0.321051\n",
      "Validation loss decreased (0.321107 --> 0.321051).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.283504 \tValidation Loss: 0.320992\n",
      "Validation loss decreased (0.321051 --> 0.320992).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.283261 \tValidation Loss: 0.320929\n",
      "Validation loss decreased (0.320992 --> 0.320929).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.283010 \tValidation Loss: 0.320865\n",
      "Validation loss decreased (0.320929 --> 0.320865).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.282743 \tValidation Loss: 0.320796\n",
      "Validation loss decreased (0.320865 --> 0.320796).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.282459 \tValidation Loss: 0.320724\n",
      "Validation loss decreased (0.320796 --> 0.320724).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.282162 \tValidation Loss: 0.320647\n",
      "Validation loss decreased (0.320724 --> 0.320647).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.281842 \tValidation Loss: 0.320563\n",
      "Validation loss decreased (0.320647 --> 0.320563).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.281496 \tValidation Loss: 0.320474\n",
      "Validation loss decreased (0.320563 --> 0.320474).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.281120 \tValidation Loss: 0.320373\n",
      "Validation loss decreased (0.320474 --> 0.320373).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.280715 \tValidation Loss: 0.320266\n",
      "Validation loss decreased (0.320373 --> 0.320266).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.280263 \tValidation Loss: 0.320144\n",
      "Validation loss decreased (0.320266 --> 0.320144).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.279756 \tValidation Loss: 0.320010\n",
      "Validation loss decreased (0.320144 --> 0.320010).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.279174 \tValidation Loss: 0.319851\n",
      "Validation loss decreased (0.320010 --> 0.319851).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.278488 \tValidation Loss: 0.319660\n",
      "Validation loss decreased (0.319851 --> 0.319660).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.277632 \tValidation Loss: 0.319414\n",
      "Validation loss decreased (0.319660 --> 0.319414).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.276475 \tValidation Loss: 0.319065\n",
      "Validation loss decreased (0.319414 --> 0.319065).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.274612 \tValidation Loss: 0.318447\n",
      "Validation loss decreased (0.319065 --> 0.318447).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.269972 \tValidation Loss: 0.316061\n",
      "Validation loss decreased (0.318447 --> 0.316061).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 1.240116 \tValidation Loss: 0.303339\n",
      "Validation loss decreased (0.316061 --> 0.303339).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 1.189503 \tValidation Loss: 0.294290\n",
      "Validation loss decreased (0.303339 --> 0.294290).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 1.158208 \tValidation Loss: 0.280646\n",
      "Validation loss decreased (0.294290 --> 0.280646).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 1.131312 \tValidation Loss: 0.287574\n",
      "Epoch: 54 \tTraining Loss: 1.105968 \tValidation Loss: 0.273260\n",
      "Validation loss decreased (0.280646 --> 0.273260).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 1.090948 \tValidation Loss: 0.264989\n",
      "Validation loss decreased (0.273260 --> 0.264989).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 1.079139 \tValidation Loss: 0.271201\n",
      "Epoch: 57 \tTraining Loss: 1.068071 \tValidation Loss: 0.259775\n",
      "Validation loss decreased (0.264989 --> 0.259775).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 1.055752 \tValidation Loss: 0.266826\n",
      "Epoch: 59 \tTraining Loss: 1.043750 \tValidation Loss: 0.256900\n",
      "Validation loss decreased (0.259775 --> 0.256900).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 1.034002 \tValidation Loss: 0.251311\n",
      "Validation loss decreased (0.256900 --> 0.251311).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 1.030592 \tValidation Loss: 0.260324\n",
      "Epoch: 62 \tTraining Loss: 1.019257 \tValidation Loss: 0.259728\n",
      "Epoch: 63 \tTraining Loss: 1.015429 \tValidation Loss: 0.253378\n",
      "Epoch: 64 \tTraining Loss: 1.009354 \tValidation Loss: 0.276417\n",
      "Epoch: 65 \tTraining Loss: 1.002478 \tValidation Loss: 0.247540\n",
      "Validation loss decreased (0.251311 --> 0.247540).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 0.997006 \tValidation Loss: 0.245700\n",
      "Validation loss decreased (0.247540 --> 0.245700).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 0.991932 \tValidation Loss: 0.253601\n",
      "Epoch: 68 \tTraining Loss: 0.984552 \tValidation Loss: 0.251511\n",
      "Epoch: 69 \tTraining Loss: 0.982645 \tValidation Loss: 0.243213\n",
      "Validation loss decreased (0.245700 --> 0.243213).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 0.974676 \tValidation Loss: 0.259262\n",
      "Epoch: 71 \tTraining Loss: 0.971620 \tValidation Loss: 0.237583\n",
      "Validation loss decreased (0.243213 --> 0.237583).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 0.971615 \tValidation Loss: 0.245837\n",
      "Epoch: 73 \tTraining Loss: 0.967250 \tValidation Loss: 0.244364\n",
      "Epoch: 74 \tTraining Loss: 0.963155 \tValidation Loss: 0.239088\n",
      "Epoch: 75 \tTraining Loss: 0.959306 \tValidation Loss: 0.246148\n",
      "Epoch: 76 \tTraining Loss: 0.954288 \tValidation Loss: 0.233712\n",
      "Validation loss decreased (0.237583 --> 0.233712).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 0.951398 \tValidation Loss: 0.234757\n",
      "Epoch: 78 \tTraining Loss: 0.945942 \tValidation Loss: 0.231624\n",
      "Validation loss decreased (0.233712 --> 0.231624).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 0.945500 \tValidation Loss: 0.231455\n",
      "Validation loss decreased (0.231624 --> 0.231455).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 0.943542 \tValidation Loss: 0.233064\n",
      "Epoch: 81 \tTraining Loss: 0.936307 \tValidation Loss: 0.233948\n",
      "Epoch: 82 \tTraining Loss: 0.939050 \tValidation Loss: 0.229369\n",
      "Validation loss decreased (0.231455 --> 0.229369).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 0.934525 \tValidation Loss: 0.232081\n",
      "Epoch: 84 \tTraining Loss: 0.929741 \tValidation Loss: 0.227969\n",
      "Validation loss decreased (0.229369 --> 0.227969).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 0.932190 \tValidation Loss: 0.227221\n",
      "Validation loss decreased (0.227969 --> 0.227221).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 0.926006 \tValidation Loss: 0.231358\n",
      "Epoch: 87 \tTraining Loss: 0.923973 \tValidation Loss: 0.227742\n",
      "Epoch: 88 \tTraining Loss: 0.921580 \tValidation Loss: 0.229076\n",
      "Epoch: 89 \tTraining Loss: 0.923507 \tValidation Loss: 0.236910\n",
      "Epoch: 90 \tTraining Loss: 0.919878 \tValidation Loss: 0.225733\n",
      "Validation loss decreased (0.227221 --> 0.225733).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 0.918799 \tValidation Loss: 0.232710\n",
      "Epoch: 92 \tTraining Loss: 0.917578 \tValidation Loss: 0.223989\n",
      "Validation loss decreased (0.225733 --> 0.223989).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 0.909126 \tValidation Loss: 0.231377\n",
      "Epoch: 94 \tTraining Loss: 0.908027 \tValidation Loss: 0.254803\n",
      "Epoch: 95 \tTraining Loss: 0.905279 \tValidation Loss: 0.224945\n",
      "Epoch: 96 \tTraining Loss: 0.907633 \tValidation Loss: 0.232085\n",
      "Epoch: 97 \tTraining Loss: 0.904948 \tValidation Loss: 0.236172\n",
      "Epoch: 98 \tTraining Loss: 0.904227 \tValidation Loss: 0.220569\n",
      "Validation loss decreased (0.223989 --> 0.220569).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 0.901072 \tValidation Loss: 0.251004\n",
      "Epoch: 100 \tTraining Loss: 0.899603 \tValidation Loss: 0.231154\n",
      "Time elapsed: 247.01 s\n"
     ]
    }
   ],
   "source": [
    "############################# Train the model_5a ############################# \n",
    "start = time.time()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model_5a.train() # prep model for training\n",
    "    for data, target in train_loader_5a:\n",
    "        # transfer data and target to GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_5a.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_5a(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(torch.long))\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_5a.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model_5a.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader_5a:\n",
    "        # transfer data and target to GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_5a(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(torch.long))\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader_5a.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader_5a.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model_5a.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        \n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0aa143db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5171\n"
     ]
    }
   ],
   "source": [
    "#############################  Calculate & Print the Accurancy_5a on Test############################# \n",
    "# Load the model with the lowest validation loss\n",
    "model_5a.load_state_dict(torch.load('model.pt'))\n",
    "# Calculate the accurancy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader_5a:\n",
    "        embeddings, labels = data\n",
    "        # transfer data and target to GPU\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        # calculating outputs by running embeddings through the network\n",
    "        model_5a.to(device)\n",
    "        outputs = model_5a(embeddings.float())\n",
    "        # the class with the highest score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3703f6",
   "metadata": {},
   "source": [
    "- (b) Repeat part (a) by considering a gated recurrent unit cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "94aa51c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################# Load the data_5b ############################# \n",
    "train_data_5b = Train(X_train_preW2Vseq20, Y_train_preW2Vseq20-1)\n",
    "test_data_5b = Test(X_test_preW2Vseq20, Y_test_preW2Vseq20-1)\n",
    "\n",
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 500\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "# obtain training indices that will be used for validation\n",
    "num_train = len(train_data_5b)\n",
    "indices = list(range(num_train))\n",
    "np.random.shuffle(indices)\n",
    "split = int(np.floor(valid_size * num_train))\n",
    "train_idx, valid_idx = indices[split:], indices[:split]\n",
    "\n",
    "# define samplers for obtaining training and validation batches\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "valid_sampler = SubsetRandomSampler(valid_idx)\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader_5b = torch.utils.data.DataLoader(train_data_5b, batch_size=batch_size, sampler=train_sampler, num_workers=num_workers,)\n",
    "valid_loader_5b = torch.utils.data.DataLoader(train_data_5b, batch_size=batch_size, sampler=valid_sampler, num_workers=num_workers)\n",
    "test_loader_5b = torch.utils.data.DataLoader(test_data_5b, batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dd036ebd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 1.297767 \tValidation Loss: 0.323643\n",
      "Validation loss decreased (inf --> 0.323643).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 1.292575 \tValidation Loss: 0.322852\n",
      "Validation loss decreased (0.323643 --> 0.322852).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 1.290530 \tValidation Loss: 0.322515\n",
      "Validation loss decreased (0.322852 --> 0.322515).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 1.289610 \tValidation Loss: 0.322346\n",
      "Validation loss decreased (0.322515 --> 0.322346).  Saving model ...\n",
      "Epoch: 5 \tTraining Loss: 1.289103 \tValidation Loss: 0.322238\n",
      "Validation loss decreased (0.322346 --> 0.322238).  Saving model ...\n",
      "Epoch: 6 \tTraining Loss: 1.288743 \tValidation Loss: 0.322155\n",
      "Validation loss decreased (0.322238 --> 0.322155).  Saving model ...\n",
      "Epoch: 7 \tTraining Loss: 1.288443 \tValidation Loss: 0.322082\n",
      "Validation loss decreased (0.322155 --> 0.322082).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 1.288166 \tValidation Loss: 0.322015\n",
      "Validation loss decreased (0.322082 --> 0.322015).  Saving model ...\n",
      "Epoch: 9 \tTraining Loss: 1.287900 \tValidation Loss: 0.321950\n",
      "Validation loss decreased (0.322015 --> 0.321950).  Saving model ...\n",
      "Epoch: 10 \tTraining Loss: 1.287641 \tValidation Loss: 0.321886\n",
      "Validation loss decreased (0.321950 --> 0.321886).  Saving model ...\n",
      "Epoch: 11 \tTraining Loss: 1.287386 \tValidation Loss: 0.321823\n",
      "Validation loss decreased (0.321886 --> 0.321823).  Saving model ...\n",
      "Epoch: 12 \tTraining Loss: 1.287136 \tValidation Loss: 0.321762\n",
      "Validation loss decreased (0.321823 --> 0.321762).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 1.286889 \tValidation Loss: 0.321701\n",
      "Validation loss decreased (0.321762 --> 0.321701).  Saving model ...\n",
      "Epoch: 14 \tTraining Loss: 1.286645 \tValidation Loss: 0.321640\n",
      "Validation loss decreased (0.321701 --> 0.321640).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 1.286404 \tValidation Loss: 0.321581\n",
      "Validation loss decreased (0.321640 --> 0.321581).  Saving model ...\n",
      "Epoch: 16 \tTraining Loss: 1.286163 \tValidation Loss: 0.321522\n",
      "Validation loss decreased (0.321581 --> 0.321522).  Saving model ...\n",
      "Epoch: 17 \tTraining Loss: 1.285925 \tValidation Loss: 0.321463\n",
      "Validation loss decreased (0.321522 --> 0.321463).  Saving model ...\n",
      "Epoch: 18 \tTraining Loss: 1.285687 \tValidation Loss: 0.321405\n",
      "Validation loss decreased (0.321463 --> 0.321405).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 1.285451 \tValidation Loss: 0.321347\n",
      "Validation loss decreased (0.321405 --> 0.321347).  Saving model ...\n",
      "Epoch: 20 \tTraining Loss: 1.285214 \tValidation Loss: 0.321289\n",
      "Validation loss decreased (0.321347 --> 0.321289).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 1.284979 \tValidation Loss: 0.321231\n",
      "Validation loss decreased (0.321289 --> 0.321231).  Saving model ...\n",
      "Epoch: 22 \tTraining Loss: 1.284742 \tValidation Loss: 0.321174\n",
      "Validation loss decreased (0.321231 --> 0.321174).  Saving model ...\n",
      "Epoch: 23 \tTraining Loss: 1.284503 \tValidation Loss: 0.321115\n",
      "Validation loss decreased (0.321174 --> 0.321115).  Saving model ...\n",
      "Epoch: 24 \tTraining Loss: 1.284265 \tValidation Loss: 0.321056\n",
      "Validation loss decreased (0.321115 --> 0.321056).  Saving model ...\n",
      "Epoch: 25 \tTraining Loss: 1.284026 \tValidation Loss: 0.320998\n",
      "Validation loss decreased (0.321056 --> 0.320998).  Saving model ...\n",
      "Epoch: 26 \tTraining Loss: 1.283784 \tValidation Loss: 0.320937\n",
      "Validation loss decreased (0.320998 --> 0.320937).  Saving model ...\n",
      "Epoch: 27 \tTraining Loss: 1.283543 \tValidation Loss: 0.320878\n",
      "Validation loss decreased (0.320937 --> 0.320878).  Saving model ...\n",
      "Epoch: 28 \tTraining Loss: 1.283300 \tValidation Loss: 0.320818\n",
      "Validation loss decreased (0.320878 --> 0.320818).  Saving model ...\n",
      "Epoch: 29 \tTraining Loss: 1.283052 \tValidation Loss: 0.320757\n",
      "Validation loss decreased (0.320818 --> 0.320757).  Saving model ...\n",
      "Epoch: 30 \tTraining Loss: 1.282800 \tValidation Loss: 0.320695\n",
      "Validation loss decreased (0.320757 --> 0.320695).  Saving model ...\n",
      "Epoch: 31 \tTraining Loss: 1.282548 \tValidation Loss: 0.320633\n",
      "Validation loss decreased (0.320695 --> 0.320633).  Saving model ...\n",
      "Epoch: 32 \tTraining Loss: 1.282291 \tValidation Loss: 0.320569\n",
      "Validation loss decreased (0.320633 --> 0.320569).  Saving model ...\n",
      "Epoch: 33 \tTraining Loss: 1.282033 \tValidation Loss: 0.320505\n",
      "Validation loss decreased (0.320569 --> 0.320505).  Saving model ...\n",
      "Epoch: 34 \tTraining Loss: 1.281769 \tValidation Loss: 0.320441\n",
      "Validation loss decreased (0.320505 --> 0.320441).  Saving model ...\n",
      "Epoch: 35 \tTraining Loss: 1.281500 \tValidation Loss: 0.320375\n",
      "Validation loss decreased (0.320441 --> 0.320375).  Saving model ...\n",
      "Epoch: 36 \tTraining Loss: 1.281229 \tValidation Loss: 0.320305\n",
      "Validation loss decreased (0.320375 --> 0.320305).  Saving model ...\n",
      "Epoch: 37 \tTraining Loss: 1.280953 \tValidation Loss: 0.320237\n",
      "Validation loss decreased (0.320305 --> 0.320237).  Saving model ...\n",
      "Epoch: 38 \tTraining Loss: 1.280669 \tValidation Loss: 0.320167\n",
      "Validation loss decreased (0.320237 --> 0.320167).  Saving model ...\n",
      "Epoch: 39 \tTraining Loss: 1.280384 \tValidation Loss: 0.320096\n",
      "Validation loss decreased (0.320167 --> 0.320096).  Saving model ...\n",
      "Epoch: 40 \tTraining Loss: 1.280093 \tValidation Loss: 0.320024\n",
      "Validation loss decreased (0.320096 --> 0.320024).  Saving model ...\n",
      "Epoch: 41 \tTraining Loss: 1.279795 \tValidation Loss: 0.319950\n",
      "Validation loss decreased (0.320024 --> 0.319950).  Saving model ...\n",
      "Epoch: 42 \tTraining Loss: 1.279491 \tValidation Loss: 0.319874\n",
      "Validation loss decreased (0.319950 --> 0.319874).  Saving model ...\n",
      "Epoch: 43 \tTraining Loss: 1.279180 \tValidation Loss: 0.319796\n",
      "Validation loss decreased (0.319874 --> 0.319796).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 1.278862 \tValidation Loss: 0.319716\n",
      "Validation loss decreased (0.319796 --> 0.319716).  Saving model ...\n",
      "Epoch: 45 \tTraining Loss: 1.278537 \tValidation Loss: 0.319633\n",
      "Validation loss decreased (0.319716 --> 0.319633).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 1.278205 \tValidation Loss: 0.319550\n",
      "Validation loss decreased (0.319633 --> 0.319550).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 1.277864 \tValidation Loss: 0.319465\n",
      "Validation loss decreased (0.319550 --> 0.319465).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 1.277513 \tValidation Loss: 0.319378\n",
      "Validation loss decreased (0.319465 --> 0.319378).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 1.277155 \tValidation Loss: 0.319288\n",
      "Validation loss decreased (0.319378 --> 0.319288).  Saving model ...\n",
      "Epoch: 50 \tTraining Loss: 1.276788 \tValidation Loss: 0.319195\n",
      "Validation loss decreased (0.319288 --> 0.319195).  Saving model ...\n",
      "Epoch: 51 \tTraining Loss: 1.276410 \tValidation Loss: 0.319100\n",
      "Validation loss decreased (0.319195 --> 0.319100).  Saving model ...\n",
      "Epoch: 52 \tTraining Loss: 1.276017 \tValidation Loss: 0.319004\n",
      "Validation loss decreased (0.319100 --> 0.319004).  Saving model ...\n",
      "Epoch: 53 \tTraining Loss: 1.275620 \tValidation Loss: 0.318903\n",
      "Validation loss decreased (0.319004 --> 0.318903).  Saving model ...\n",
      "Epoch: 54 \tTraining Loss: 1.275206 \tValidation Loss: 0.318797\n",
      "Validation loss decreased (0.318903 --> 0.318797).  Saving model ...\n",
      "Epoch: 55 \tTraining Loss: 1.274782 \tValidation Loss: 0.318689\n",
      "Validation loss decreased (0.318797 --> 0.318689).  Saving model ...\n",
      "Epoch: 56 \tTraining Loss: 1.274345 \tValidation Loss: 0.318578\n",
      "Validation loss decreased (0.318689 --> 0.318578).  Saving model ...\n",
      "Epoch: 57 \tTraining Loss: 1.273890 \tValidation Loss: 0.318462\n",
      "Validation loss decreased (0.318578 --> 0.318462).  Saving model ...\n",
      "Epoch: 58 \tTraining Loss: 1.273422 \tValidation Loss: 0.318342\n",
      "Validation loss decreased (0.318462 --> 0.318342).  Saving model ...\n",
      "Epoch: 59 \tTraining Loss: 1.272934 \tValidation Loss: 0.318217\n",
      "Validation loss decreased (0.318342 --> 0.318217).  Saving model ...\n",
      "Epoch: 60 \tTraining Loss: 1.272431 \tValidation Loss: 0.318089\n",
      "Validation loss decreased (0.318217 --> 0.318089).  Saving model ...\n",
      "Epoch: 61 \tTraining Loss: 1.271908 \tValidation Loss: 0.317955\n",
      "Validation loss decreased (0.318089 --> 0.317955).  Saving model ...\n",
      "Epoch: 62 \tTraining Loss: 1.271359 \tValidation Loss: 0.317812\n",
      "Validation loss decreased (0.317955 --> 0.317812).  Saving model ...\n",
      "Epoch: 63 \tTraining Loss: 1.270788 \tValidation Loss: 0.317665\n",
      "Validation loss decreased (0.317812 --> 0.317665).  Saving model ...\n",
      "Epoch: 64 \tTraining Loss: 1.270194 \tValidation Loss: 0.317512\n",
      "Validation loss decreased (0.317665 --> 0.317512).  Saving model ...\n",
      "Epoch: 65 \tTraining Loss: 1.269567 \tValidation Loss: 0.317351\n",
      "Validation loss decreased (0.317512 --> 0.317351).  Saving model ...\n",
      "Epoch: 66 \tTraining Loss: 1.268908 \tValidation Loss: 0.317181\n",
      "Validation loss decreased (0.317351 --> 0.317181).  Saving model ...\n",
      "Epoch: 67 \tTraining Loss: 1.268212 \tValidation Loss: 0.317002\n",
      "Validation loss decreased (0.317181 --> 0.317002).  Saving model ...\n",
      "Epoch: 68 \tTraining Loss: 1.267476 \tValidation Loss: 0.316808\n",
      "Validation loss decreased (0.317002 --> 0.316808).  Saving model ...\n",
      "Epoch: 69 \tTraining Loss: 1.266692 \tValidation Loss: 0.316601\n",
      "Validation loss decreased (0.316808 --> 0.316601).  Saving model ...\n",
      "Epoch: 70 \tTraining Loss: 1.265851 \tValidation Loss: 0.316382\n",
      "Validation loss decreased (0.316601 --> 0.316382).  Saving model ...\n",
      "Epoch: 71 \tTraining Loss: 1.264945 \tValidation Loss: 0.316140\n",
      "Validation loss decreased (0.316382 --> 0.316140).  Saving model ...\n",
      "Epoch: 72 \tTraining Loss: 1.263962 \tValidation Loss: 0.315879\n",
      "Validation loss decreased (0.316140 --> 0.315879).  Saving model ...\n",
      "Epoch: 73 \tTraining Loss: 1.262887 \tValidation Loss: 0.315592\n",
      "Validation loss decreased (0.315879 --> 0.315592).  Saving model ...\n",
      "Epoch: 74 \tTraining Loss: 1.261696 \tValidation Loss: 0.315279\n",
      "Validation loss decreased (0.315592 --> 0.315279).  Saving model ...\n",
      "Epoch: 75 \tTraining Loss: 1.260366 \tValidation Loss: 0.314914\n",
      "Validation loss decreased (0.315279 --> 0.314914).  Saving model ...\n",
      "Epoch: 76 \tTraining Loss: 1.258842 \tValidation Loss: 0.314495\n",
      "Validation loss decreased (0.314914 --> 0.314495).  Saving model ...\n",
      "Epoch: 77 \tTraining Loss: 1.257090 \tValidation Loss: 0.314011\n",
      "Validation loss decreased (0.314495 --> 0.314011).  Saving model ...\n",
      "Epoch: 78 \tTraining Loss: 1.255000 \tValidation Loss: 0.313431\n",
      "Validation loss decreased (0.314011 --> 0.313431).  Saving model ...\n",
      "Epoch: 79 \tTraining Loss: 1.252431 \tValidation Loss: 0.312698\n",
      "Validation loss decreased (0.313431 --> 0.312698).  Saving model ...\n",
      "Epoch: 80 \tTraining Loss: 1.249130 \tValidation Loss: 0.311736\n",
      "Validation loss decreased (0.312698 --> 0.311736).  Saving model ...\n",
      "Epoch: 81 \tTraining Loss: 1.244634 \tValidation Loss: 0.310379\n",
      "Validation loss decreased (0.311736 --> 0.310379).  Saving model ...\n",
      "Epoch: 82 \tTraining Loss: 1.237957 \tValidation Loss: 0.308279\n",
      "Validation loss decreased (0.310379 --> 0.308279).  Saving model ...\n",
      "Epoch: 83 \tTraining Loss: 1.226617 \tValidation Loss: 0.304440\n",
      "Validation loss decreased (0.308279 --> 0.304440).  Saving model ...\n",
      "Epoch: 84 \tTraining Loss: 1.203611 \tValidation Loss: 0.296006\n",
      "Validation loss decreased (0.304440 --> 0.296006).  Saving model ...\n",
      "Epoch: 85 \tTraining Loss: 1.155616 \tValidation Loss: 0.281637\n",
      "Validation loss decreased (0.296006 --> 0.281637).  Saving model ...\n",
      "Epoch: 86 \tTraining Loss: 1.107987 \tValidation Loss: 0.273769\n",
      "Validation loss decreased (0.281637 --> 0.273769).  Saving model ...\n",
      "Epoch: 87 \tTraining Loss: 1.087072 \tValidation Loss: 0.269973\n",
      "Validation loss decreased (0.273769 --> 0.269973).  Saving model ...\n",
      "Epoch: 88 \tTraining Loss: 1.074214 \tValidation Loss: 0.267203\n",
      "Validation loss decreased (0.269973 --> 0.267203).  Saving model ...\n",
      "Epoch: 89 \tTraining Loss: 1.064148 \tValidation Loss: 0.264845\n",
      "Validation loss decreased (0.267203 --> 0.264845).  Saving model ...\n",
      "Epoch: 90 \tTraining Loss: 1.055735 \tValidation Loss: 0.262873\n",
      "Validation loss decreased (0.264845 --> 0.262873).  Saving model ...\n",
      "Epoch: 91 \tTraining Loss: 1.048016 \tValidation Loss: 0.261062\n",
      "Validation loss decreased (0.262873 --> 0.261062).  Saving model ...\n",
      "Epoch: 92 \tTraining Loss: 1.041087 \tValidation Loss: 0.259545\n",
      "Validation loss decreased (0.261062 --> 0.259545).  Saving model ...\n",
      "Epoch: 93 \tTraining Loss: 1.034706 \tValidation Loss: 0.257947\n",
      "Validation loss decreased (0.259545 --> 0.257947).  Saving model ...\n",
      "Epoch: 94 \tTraining Loss: 1.028736 \tValidation Loss: 0.256612\n",
      "Validation loss decreased (0.257947 --> 0.256612).  Saving model ...\n",
      "Epoch: 95 \tTraining Loss: 1.023008 \tValidation Loss: 0.255250\n",
      "Validation loss decreased (0.256612 --> 0.255250).  Saving model ...\n",
      "Epoch: 96 \tTraining Loss: 1.017530 \tValidation Loss: 0.254148\n",
      "Validation loss decreased (0.255250 --> 0.254148).  Saving model ...\n",
      "Epoch: 97 \tTraining Loss: 1.012366 \tValidation Loss: 0.253067\n",
      "Validation loss decreased (0.254148 --> 0.253067).  Saving model ...\n",
      "Epoch: 98 \tTraining Loss: 1.007167 \tValidation Loss: 0.252144\n",
      "Validation loss decreased (0.253067 --> 0.252144).  Saving model ...\n",
      "Epoch: 99 \tTraining Loss: 1.002186 \tValidation Loss: 0.250422\n",
      "Validation loss decreased (0.252144 --> 0.250422).  Saving model ...\n",
      "Epoch: 100 \tTraining Loss: 0.997231 \tValidation Loss: 0.249797\n",
      "Validation loss decreased (0.250422 --> 0.249797).  Saving model ...\n",
      "Time elapsed: 249.07 s\n"
     ]
    }
   ],
   "source": [
    "############################# Train the model_5b ############################# \n",
    "start = time.time()\n",
    "\n",
    "# number of epochs to train the model\n",
    "n_epochs = 100\n",
    "\n",
    "# initialize tracker for minimum validation loss\n",
    "valid_loss_min = np.Inf # set initial \"min\" to infinity\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    # monitor training loss\n",
    "    train_loss = 0.0\n",
    "    valid_loss = 0.0\n",
    "    \n",
    "    ###################\n",
    "    # train the model #\n",
    "    ###################\n",
    "    model_5b.train() # prep model for training\n",
    "    for data, target in train_loader_5b:\n",
    "        # transfer data and target to GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # clear the gradients of all optimized variables\n",
    "        optimizer_5b.zero_grad()\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_5b(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(torch.long))\n",
    "        # backward pass: compute gradient of the loss with respect to model parameters\n",
    "        loss.backward()\n",
    "        # perform a single optimization step (parameter update)\n",
    "        optimizer_5b.step()\n",
    "        # update running training loss\n",
    "        train_loss += loss.item()*data.size(0)\n",
    "    \n",
    "    ######################    \n",
    "    # validate the model #\n",
    "    ######################\n",
    "    model_5b.eval() # prep model for evaluation\n",
    "    for data, target in valid_loader_5b:\n",
    "        # transfer data and target to GPU\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        # forward pass: compute predicted outputs by passing inputs to the model\n",
    "        output = model_5b(data.float())\n",
    "        # calculate the loss\n",
    "        loss = criterion(output, target.to(torch.long))\n",
    "        # update running validation loss \n",
    "        valid_loss += loss.item()*data.size(0)\n",
    "        \n",
    "    # print training/validation statistics \n",
    "    # calculate average loss over an epoch\n",
    "    train_loss = train_loss/len(train_loader_5b.dataset)\n",
    "    valid_loss = valid_loss/len(valid_loader_5b.dataset)\n",
    "    \n",
    "    print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "        epoch+1, \n",
    "        train_loss,\n",
    "        valid_loss\n",
    "        ))\n",
    "    \n",
    "    # save model if validation loss has decreased\n",
    "    if valid_loss <= valid_loss_min:\n",
    "        print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "        valid_loss_min,\n",
    "        valid_loss))\n",
    "        torch.save(model_5b.state_dict(), 'model.pt')\n",
    "        valid_loss_min = valid_loss\n",
    "        \n",
    "end = time.time()\n",
    "print('Time elapsed: %.2f s' % (end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a9891b4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4332\n"
     ]
    }
   ],
   "source": [
    "#############################  Calculate & Print the Accurancy_5b on Test############################# \n",
    "# Load the model with the lowest validation loss\n",
    "model_5b.load_state_dict(torch.load('model.pt'))\n",
    "# Calculate the accurancy\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in test_loader_5b:\n",
    "        embeddings, labels = data\n",
    "        # transfer data and target to GPU\n",
    "        embeddings, labels = embeddings.to(device), labels.to(device)\n",
    "        # calculating outputs by running embeddings through the network\n",
    "        model_5b.to(device)\n",
    "        outputs = model_5b(embeddings.float())\n",
    "        # the class with the highest score is what we choose as prediction\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b26404ce",
   "metadata": {},
   "source": [
    "- Conclusion\n",
    "    - RNN model(0.52) works worse than FNN models(0.57 / 0.59) here. \n",
    "    - GRU model(0.43) (Gated Recurrent Neural Network) even work worse than simple RNN model(0.52).\n",
    "    - The poor results of RNN may also be related to the network settings of RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0e43c5",
   "metadata": {},
   "source": [
    "# Accuracy values for 6 cases: \n",
    "1. Perceptron -> 0.46\n",
    "2. SVM -> 0.56\n",
    "3. FNN(average Word2Vec vectors) -> 0.57\n",
    "4. FNN(first 10 Word2Vec vectors) -> 0.59\n",
    "5. RNN -> 0.52\n",
    "6. GRN -> 0.43"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
